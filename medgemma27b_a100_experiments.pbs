#!/bin/bash -l
#PBS -N medgemma27b_experiments
#PBS -l select=1:ncpus=8:mem=64GB:ngpus=1:gpu_model=A100:gpu_sliced=False
#PBS -l walltime=08:00:00
#PBS -m abe
#PBS -j oe

set -euo pipefail

cd "$PBS_O_WORKDIR" || exit 1
export PATH="$HOME/.local/bin:$HOME/bin:$PATH"

# Persistent model cache on HOME (Lustre-backed on cluster)
export OLLAMA_MODELS="${OLLAMA_MODELS:-$HOME/.ollama/models}"

MODEL_IMAGE="alibayram/medgemma:27b"
MODEL_NAME="${MODEL_NAME:-ollama_medgemma_27b}"
DATA_FILE="${DATA_FILE:-./annotated_data/db_20260129_tokenised.jsonl}"
CHECKLIST_OPTIMISER="${CHECKLIST_OPTIMISER:-gepa_light_checklist}"
SBAR_OPTIMISER="${SBAR_OPTIMISER:-gepa_light_span}"
ANNOTATOR_ID="${ANNOTATOR_ID:-}"
USE_ALL="${USE_ALL:-0}"          # 1 => evaluate on all matching examples
RUN_TRAIN="${RUN_TRAIN:-1}"      # 1 => train then eval, 0 => baseline eval only
RUN_CHECKLIST="${RUN_CHECKLIST:-1}"
RUN_SBAR="${RUN_SBAR:-1}"
REFLECTION_MODEL="openai/gpt-5.2"

STAMP="$(date +%Y%m%d_%H%M%S)"
OUT_DIR="${OUT_DIR:-./cluster_runs/medgemma27b_${STAMP}}"
mkdir -p "$OUT_DIR"/compiled_programs "$OUT_DIR"/evals "$OUT_DIR"/logs
exec > >(tee -a "$OUT_DIR/logs/job.log") 2>&1

echo "Node: $(hostname)"
echo "Working directory: $PWD"
echo "Output directory: $OUT_DIR"
echo "GPU info:"
nvidia-smi

PYTHON_MODULE="${PYTHON_MODULE:-Python/3.12.3}"
PYTHON_MODULE_CANDIDATES="${PYTHON_MODULE_CANDIDATES:-$PYTHON_MODULE Python/3.11.9 Python/3.10.14 Python/3.10.13}"
PYTHON_DEP_MODULES="${PYTHON_DEP_MODULES:-GCCcore/13.3.0}"
VENV_DIR="${VENV_DIR:-$PBS_O_WORKDIR/.venv_cluster_py312}"

if ! command -v module >/dev/null 2>&1; then
  echo "Environment modules are required but 'module' command is not available."
  exit 1
fi

SELECTED_PYTHON_MODULE=""
SELECTED_PYTHON_DEP_MODULES=""
DEP_OPTIONS=()
if [ -n "$PYTHON_DEP_MODULES" ]; then
  DEP_OPTIONS+=("$PYTHON_DEP_MODULES")
fi
DEP_OPTIONS+=("")

for dep in "${DEP_OPTIONS[@]}"; do
  # shellcheck disable=SC2086
  for mod in $PYTHON_MODULE_CANDIDATES; do
    module purge || true
    if [ -n "$dep" ]; then
      # shellcheck disable=SC2086
      if ! module load $dep; then
        continue
      fi
    fi
    if ! module load "$mod"; then
      continue
    fi
    if command -v python3 >/dev/null 2>&1 && python3 -c "import sys; raise SystemExit(0 if sys.version_info >= (3, 10) else 1)" >/dev/null 2>&1; then
      SELECTED_PYTHON_MODULE="$mod"
      SELECTED_PYTHON_DEP_MODULES="$dep"
      break 2
    fi
  done
done

if [ -z "$SELECTED_PYTHON_MODULE" ]; then
  echo "Could not load a runnable python3 >=3.10 on this node."
  echo "Tried PYTHON_MODULE_CANDIDATES: $PYTHON_MODULE_CANDIDATES"
  echo "Tried dep module: ${PYTHON_DEP_MODULES:-<none>} (and none)"
  exit 1
fi

echo "Using Python module: $SELECTED_PYTHON_MODULE"
if [ -n "$SELECTED_PYTHON_DEP_MODULES" ]; then
  echo "Using dependency module(s): $SELECTED_PYTHON_DEP_MODULES"
fi

if [ ! -x "$VENV_DIR/bin/python3" ] && [ ! -x "$VENV_DIR/bin/python" ]; then
  python3 -m venv "$VENV_DIR"
fi

PYTHON_BIN="$VENV_DIR/bin/python3"
if [ ! -x "$PYTHON_BIN" ]; then
  PYTHON_BIN="$VENV_DIR/bin/python"
fi
if ! "$PYTHON_BIN" -c "import sys; raise SystemExit(0 if sys.version_info >= (3, 10) else 1)" >/dev/null 2>&1; then
  echo "Existing venv is not runnable or is <3.10; recreating $VENV_DIR"
  python3 -m venv --clear "$VENV_DIR"
  PYTHON_BIN="$VENV_DIR/bin/python3"
  "$PYTHON_BIN" -c "import sys; raise SystemExit(0 if sys.version_info >= (3, 10) else 1)" >/dev/null
fi

DEPS_STAMP="$VENV_DIR/.handover_deps_v4"
if [ ! -f "$DEPS_STAMP" ]; then
  "$PYTHON_BIN" -m pip install --upgrade pip setuptools wheel
  "$PYTHON_BIN" -m pip install \
    "dspy-ai>=3.0.4" \
    "numpy>=2.3.5" \
    "rapidfuzz>=3.14.3" \
    "spacy>=3.8.11" \
    "thinc>=8.3.10" \
    "srsly>=2.5.1"
  touch "$DEPS_STAMP"
fi

export PYTHONPATH="$PWD/src${PYTHONPATH:+:$PYTHONPATH}"
PY_RUNNER=("$PYTHON_BIN")
if ! command -v ollama >/dev/null 2>&1; then
  echo "ollama is not available on PATH."
  exit 1
fi

# Pick a random free local port for this job's Ollama daemon.
while :; do
  PORT=$((11434 + RANDOM % 5000))
  if ! nc -z 127.0.0.1 "$PORT" 2>/dev/null; then
    break
  fi
done

export OLLAMA_HOST="127.0.0.1:${PORT}"
export OLLAMA_API_BASE="http://${OLLAMA_HOST}"
echo "Using OLLAMA_HOST=${OLLAMA_HOST}"
echo "Using OLLAMA_API_BASE=${OLLAMA_API_BASE}"

ollama serve >/dev/null 2>&1 &
OLLAMA_PID=$!
trap 'kill "$OLLAMA_PID" 2>/dev/null || true' EXIT

for i in {1..90}; do
  if curl -fs "http://${OLLAMA_HOST}/api/tags" >/dev/null; then
    break
  fi
  sleep 1
  if [ "$i" -eq 90 ]; then
    echo "Ollama did not start within 90 seconds."
    exit 1
  fi
done

if ! ollama show "$MODEL_IMAGE" >/dev/null 2>&1; then
  echo "Pulling ${MODEL_IMAGE}..."
  ollama pull "$MODEL_IMAGE"
else
  echo "Model ${MODEL_IMAGE} already available in local cache."
fi

echo "Available models:"
ollama list

COMMON_ARGS=(--data-file "$DATA_FILE" --model-name "$MODEL_NAME")
if [ -n "$ANNOTATOR_ID" ]; then
  COMMON_ARGS+=(--annotator-id "$ANNOTATOR_ID")
fi
EVAL_EXTRA_ARGS=()
if [ "$USE_ALL" = "1" ]; then
  EVAL_EXTRA_ARGS+=(--use-all)
fi

echo "Run configuration:"
echo "  RUN_CHECKLIST=$RUN_CHECKLIST"
echo "  RUN_SBAR=$RUN_SBAR"
echo "  RUN_TRAIN=$RUN_TRAIN"
echo "  CHECKLIST_OPTIMISER=$CHECKLIST_OPTIMISER"
echo "  SBAR_OPTIMISER=$SBAR_OPTIMISER"
echo "  ANNOTATOR_ID=${ANNOTATOR_ID:-<all>}"
echo "  USE_ALL=$USE_ALL"

if [ "$RUN_TRAIN" = "1" ]; then
  NEED_REFLECTION=0
  if [[ "$CHECKLIST_OPTIMISER" == gepa_* ]] && [ "$RUN_CHECKLIST" = "1" ]; then
    NEED_REFLECTION=1
  fi
  if [[ "$SBAR_OPTIMISER" == gepa_* ]] && [ "$RUN_SBAR" = "1" ]; then
    NEED_REFLECTION=1
  fi

  if [ "$NEED_REFLECTION" = "1" ]; then
    if [ -z "${OPENAI_API_KEY:-}" ]; then
      echo "OPENAI_API_KEY is required for GEPA reflection (reflection model: $REFLECTION_MODEL)."
      exit 1
    fi
    echo "GEPA reflection model: $REFLECTION_MODEL"
  fi
fi

if [ "$RUN_CHECKLIST" = "1" ]; then
  CHECKLIST_MODEL_FILE="$OUT_DIR/compiled_programs/checklist_${MODEL_NAME}_${CHECKLIST_OPTIMISER}.json"
  CHECKLIST_EVAL_FILE="$OUT_DIR/evals/eval_checklist_${MODEL_NAME}_${CHECKLIST_OPTIMISER}.jsonl"

  echo "=== CHECKLIST TASK ==="
  if [ "$RUN_TRAIN" = "1" ]; then
    "${PY_RUNNER[@]}" run_train_checklist.py \
      "${COMMON_ARGS[@]}" \
      --optimiser-name "$CHECKLIST_OPTIMISER" \
      --output-model-file "$CHECKLIST_MODEL_FILE"

    "${PY_RUNNER[@]}" run_eval_checklist.py \
      "${COMMON_ARGS[@]}" \
      --output-model-file "$CHECKLIST_MODEL_FILE" \
      --eval-results-file "$CHECKLIST_EVAL_FILE" \
      "${EVAL_EXTRA_ARGS[@]}"
  else
    "${PY_RUNNER[@]}" run_eval_checklist.py \
      "${COMMON_ARGS[@]}" \
      --baseline \
      --eval-results-file "$CHECKLIST_EVAL_FILE" \
      "${EVAL_EXTRA_ARGS[@]}"
  fi
fi

if [ "$RUN_SBAR" = "1" ]; then
  SBAR_MODEL_FILE="$OUT_DIR/compiled_programs/sbar_${MODEL_NAME}_${SBAR_OPTIMISER}.json"
  SBAR_EVAL_FILE="$OUT_DIR/evals/eval_sbar_${MODEL_NAME}_${SBAR_OPTIMISER}.jsonl"

  echo "=== SBAR SPAN TASK ==="
  if [ "$RUN_TRAIN" = "1" ]; then
    "${PY_RUNNER[@]}" run_train_sbar_span.py \
      "${COMMON_ARGS[@]}" \
      --optimiser-name "$SBAR_OPTIMISER" \
      --output-model-file "$SBAR_MODEL_FILE"

    "${PY_RUNNER[@]}" run_eval_sbar_span.py \
      "${COMMON_ARGS[@]}" \
      --output-model-file "$SBAR_MODEL_FILE" \
      --eval-results-file "$SBAR_EVAL_FILE" \
      "${EVAL_EXTRA_ARGS[@]}"
  else
    "${PY_RUNNER[@]}" run_eval_sbar_span.py \
      "${COMMON_ARGS[@]}" \
      --baseline \
      --eval-results-file "$SBAR_EVAL_FILE" \
      "${EVAL_EXTRA_ARGS[@]}"
  fi
fi

echo "All requested experiments completed."
echo "Artifacts:"
echo "  Models: $OUT_DIR/compiled_programs"
echo "  Evals:  $OUT_DIR/evals"
echo "  Logs:   $OUT_DIR/logs/job.log"
